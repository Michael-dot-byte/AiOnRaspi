{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USSV_OlCFKOD"
   },
   "source": [
    "# Training a neural network on MNIST with Keras\n",
    "\n",
    "This simple example demonstrates how to plug TensorFlow Datasets (TFDS) into a Keras model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8y9ZkLXmAZc"
   },
   "source": [
    "Copyright 2020 The TensorFlow Datasets Authors, Licensed under the Apache License, Version 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGw9EgE0tC0C"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/datasets/keras_example\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/keras_example.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/datasets/blob/master/docs/keras_example.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/datasets/docs/keras_example.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TTBSvHcSLBzc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 16:37:52.965600: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-18 16:37:52.965806: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/tim/Dokumente/Projects/jupyterLab/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjI6VgOBf0v0"
   },
   "source": [
    "## Step 1: Create your input pipeline\n",
    "\n",
    "Start by building an efficient input pipeline using advices from:\n",
    "* The [Performance tips](https://www.tensorflow.org/datasets/performances) guide\n",
    "* The [Better performance with the `tf.data` API](https://www.tensorflow.org/guide/data_performance#optimize_performance) guide\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3aH3vP_XLI8"
   },
   "source": [
    "### Load a dataset\n",
    "\n",
    "Load the MNIST dataset with the following arguments:\n",
    "\n",
    "* `shuffle_files=True`: The MNIST data is only stored in a single file, but for larger datasets with multiple files on disk, it's good practice to shuffle them when training.\n",
    "* `as_supervised=True`: Returns a tuple `(img, label)` instead of a dictionary `{'image': img, 'label': label}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZUMhCXhFXdHQ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 16:38:03.465303: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-18 16:38:03.465505: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-18 16:38:03.465573: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (VirtDebian): /proc/driver/nvidia/version does not exist\n",
      "2022-08-18 16:38:03.480070: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'emnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgwCFAcWXQTx"
   },
   "source": [
    "### Build a training pipeline\n",
    "\n",
    "Apply the following transformations:\n",
    "\n",
    "* `tf.data.Dataset.map`: TFDS provide images of type `tf.uint8`, while the model expects `tf.float32`. Therefore, you need to normalize images.\n",
    "* `tf.data.Dataset.cache` As you fit the dataset in memory, cache it before shuffling for a better performance.<br/>\n",
    "__Note:__ Random transformations should be applied after caching.\n",
    "* `tf.data.Dataset.shuffle`: For true randomness, set the shuffle buffer to the full dataset size.<br/>\n",
    "__Note:__ For large datasets that can't fit in memory, use `buffer_size=1000` if your system allows it.\n",
    "* `tf.data.Dataset.batch`: Batch elements of the dataset after shuffling to get unique batches at each epoch.\n",
    "* `tf.data.Dataset.prefetch`: It is good practice to end the pipeline by prefetching [for performance](https://www.tensorflow.org/guide/data_performance#prefetching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "haykx2K9XgiI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbsMy4X1XVFv"
   },
   "source": [
    "### Build an evaluation pipeline\n",
    "\n",
    "Your testing pipeline is similar to the training pipeline with small differences:\n",
    "\n",
    " * You don't need to call `tf.data.Dataset.shuffle`.\n",
    " * Caching is done after batching because batches can be the same between epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "A0KjuDf7XiqY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 16:38:33.311060: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 19586 of 697932\n",
      "2022-08-18 16:38:43.311716: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 42484 of 697932\n",
      "2022-08-18 16:38:53.311188: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 65563 of 697932\n",
      "2022-08-18 16:39:03.311150: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 85622 of 697932\n",
      "2022-08-18 16:39:13.311209: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 106692 of 697932\n",
      "2022-08-18 16:39:23.311013: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 126841 of 697932\n",
      "2022-08-18 16:39:33.311043: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 146483 of 697932\n",
      "2022-08-18 16:39:43.310949: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 167840 of 697932\n",
      "2022-08-18 16:39:53.311467: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 189108 of 697932\n",
      "2022-08-18 16:40:03.312743: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 209050 of 697932\n",
      "2022-08-18 16:40:13.311074: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 229539 of 697932\n",
      "2022-08-18 16:40:23.310871: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 249315 of 697932\n",
      "2022-08-18 16:40:33.310942: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 268808 of 697932\n",
      "2022-08-18 16:40:43.311736: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 288881 of 697932\n",
      "2022-08-18 16:40:53.311205: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 309742 of 697932\n",
      "2022-08-18 16:41:03.311815: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 330892 of 697932\n",
      "2022-08-18 16:41:13.310983: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 352239 of 697932\n",
      "2022-08-18 16:41:23.311072: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 371733 of 697932\n",
      "2022-08-18 16:41:33.312136: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 392072 of 697932\n",
      "2022-08-18 16:41:43.311314: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 412774 of 697932\n",
      "2022-08-18 16:41:53.313327: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 432911 of 697932\n",
      "2022-08-18 16:42:03.311394: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 453775 of 697932\n",
      "2022-08-18 16:42:13.310919: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 474971 of 697932\n",
      "2022-08-18 16:42:23.312134: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 495190 of 697932\n",
      "2022-08-18 16:42:33.310837: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 515390 of 697932\n",
      "2022-08-18 16:42:43.311874: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 536258 of 697932\n",
      "2022-08-18 16:42:53.311885: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 556961 of 697932\n",
      "2022-08-18 16:43:03.310880: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 577108 of 697932\n",
      "2022-08-18 16:43:13.311053: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 598016 of 697932\n",
      "2022-08-18 16:43:23.310848: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 619220 of 697932\n",
      "2022-08-18 16:43:33.311046: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 641159 of 697932\n",
      "2022-08-18 16:43:43.311018: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 663706 of 697932\n",
      "2022-08-18 16:43:53.312435: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 683706 of 697932\n",
      "2022-08-18 16:44:00.332055: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:415] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "(28, 28, 1)\n",
      "(128,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 H\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "charList = ['0','1','2','3','4','5','6','7','8','9',\n",
    "'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z',\n",
    "'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "\n",
    "\n",
    "#print(ds_train.inspect)\n",
    "#dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "for element in ds_train:\n",
    "    #print(element)\n",
    "    break\n",
    "for element in ds_train.as_numpy_iterator():\n",
    "    print(type(element))\n",
    "    element1, element2 = element\n",
    "    print(element1[0].shape)\n",
    "    print(element2.shape)\n",
    "    plt.imshow(element1[0], cmap='gray') #Bilder sind gedreht und gespiegelt\n",
    "    plt.show()\n",
    "    print(element2[0], charList[element2[0]])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTFoji3INMEM"
   },
   "source": [
    "## Step 2: Create and train the model\n",
    "\n",
    "Plug the TFDS input pipeline into a simple Keras model, compile the model, and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XWqxdmS1NLKA",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "5452/5453 [============================>.] - ETA: 0s - loss: 0.7731 - sparse_categorical_accuracy: 0.7683"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 16:48:29.409093: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tfKerasCharsEMNISTSoftmax/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tfKerasCharsEMNISTSoftmax/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5453/5453 [==============================] - 124s 22ms/step - loss: 0.7731 - sparse_categorical_accuracy: 0.7683 - val_loss: 0.5987 - val_sparse_categorical_accuracy: 0.8088\n",
      "Epoch 2/30\n",
      "5453/5453 [==============================] - ETA: 0s - loss: 0.5741 - sparse_categorical_accuracy: 0.8157INFO:tensorflow:Assets written to: tfKerasCharsEMNISTSoftmax/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tfKerasCharsEMNISTSoftmax/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5453/5453 [==============================] - 77s 14ms/step - loss: 0.5741 - sparse_categorical_accuracy: 0.8157 - val_loss: 0.5688 - val_sparse_categorical_accuracy: 0.8185\n",
      "Epoch 3/30\n",
      "5450/5453 [============================>.] - ETA: 0s - loss: 0.5517 - sparse_categorical_accuracy: 0.8219INFO:tensorflow:Assets written to: tfKerasCharsEMNISTSoftmax/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tfKerasCharsEMNISTSoftmax/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5453/5453 [==============================] - 72s 13ms/step - loss: 0.5517 - sparse_categorical_accuracy: 0.8219 - val_loss: 0.5568 - val_sparse_categorical_accuracy: 0.8230\n",
      "Epoch 4/30\n",
      "5453/5453 [==============================] - 71s 13ms/step - loss: 0.5458 - sparse_categorical_accuracy: 0.8242 - val_loss: 0.5642 - val_sparse_categorical_accuracy: 0.8208\n",
      "Epoch 5/30\n",
      "5453/5453 [==============================] - 70s 13ms/step - loss: 0.5474 - sparse_categorical_accuracy: 0.8245 - val_loss: 0.5996 - val_sparse_categorical_accuracy: 0.8043\n",
      "Epoch 6/30\n",
      "5453/5453 [==============================] - 70s 13ms/step - loss: 0.5512 - sparse_categorical_accuracy: 0.8237 - val_loss: 0.5856 - val_sparse_categorical_accuracy: 0.8215\n",
      "Epoch 7/30\n",
      "5453/5453 [==============================] - 73s 13ms/step - loss: 0.5571 - sparse_categorical_accuracy: 0.8228 - val_loss: 0.6016 - val_sparse_categorical_accuracy: 0.8114\n",
      "Epoch 8/30\n",
      "5453/5453 [==============================] - 70s 13ms/step - loss: 0.5638 - sparse_categorical_accuracy: 0.8223 - val_loss: 0.6177 - val_sparse_categorical_accuracy: 0.8111\n",
      "Epoch 9/30\n",
      "5453/5453 [==============================] - 72s 13ms/step - loss: 0.5699 - sparse_categorical_accuracy: 0.8208 - val_loss: 0.6147 - val_sparse_categorical_accuracy: 0.8103\n",
      "Epoch 10/30\n",
      "5453/5453 [==============================] - 69s 12ms/step - loss: 0.5770 - sparse_categorical_accuracy: 0.8197 - val_loss: 0.6344 - val_sparse_categorical_accuracy: 0.8113\n",
      "Epoch 11/30\n",
      "5453/5453 [==============================] - 79s 14ms/step - loss: 0.5836 - sparse_categorical_accuracy: 0.8186 - val_loss: 0.6423 - val_sparse_categorical_accuracy: 0.8057\n",
      "Epoch 12/30\n",
      "5453/5453 [==============================] - 86s 16ms/step - loss: 0.5902 - sparse_categorical_accuracy: 0.8180 - val_loss: 0.6381 - val_sparse_categorical_accuracy: 0.8104\n",
      "Epoch 13/30\n",
      "5453/5453 [==============================] - 92s 16ms/step - loss: 0.5975 - sparse_categorical_accuracy: 0.8170 - val_loss: 0.6589 - val_sparse_categorical_accuracy: 0.8108\n",
      "Epoch 14/30\n",
      "5453/5453 [==============================] - 89s 16ms/step - loss: 0.6046 - sparse_categorical_accuracy: 0.8156 - val_loss: 0.6789 - val_sparse_categorical_accuracy: 0.8005\n",
      "Epoch 15/30\n",
      "5453/5453 [==============================] - 73s 13ms/step - loss: 0.6099 - sparse_categorical_accuracy: 0.8147 - val_loss: 0.6786 - val_sparse_categorical_accuracy: 0.8054\n",
      "Epoch 16/30\n",
      "5453/5453 [==============================] - 88s 16ms/step - loss: 0.6173 - sparse_categorical_accuracy: 0.8137 - val_loss: 0.6963 - val_sparse_categorical_accuracy: 0.7984\n",
      "Epoch 17/30\n",
      "5453/5453 [==============================] - 79s 14ms/step - loss: 0.6219 - sparse_categorical_accuracy: 0.8124 - val_loss: 0.7020 - val_sparse_categorical_accuracy: 0.8036\n",
      "Epoch 18/30\n",
      "5453/5453 [==============================] - 93s 17ms/step - loss: 0.6283 - sparse_categorical_accuracy: 0.8121 - val_loss: 0.6882 - val_sparse_categorical_accuracy: 0.8013\n",
      "Epoch 19/30\n",
      "5453/5453 [==============================] - 95s 17ms/step - loss: 0.6343 - sparse_categorical_accuracy: 0.8114 - val_loss: 0.7180 - val_sparse_categorical_accuracy: 0.7987\n",
      "Epoch 20/30\n",
      "5453/5453 [==============================] - 84s 15ms/step - loss: 0.6396 - sparse_categorical_accuracy: 0.8102 - val_loss: 0.7208 - val_sparse_categorical_accuracy: 0.8029\n",
      "Epoch 21/30\n",
      "5453/5453 [==============================] - 79s 14ms/step - loss: 0.6446 - sparse_categorical_accuracy: 0.8095 - val_loss: 0.7267 - val_sparse_categorical_accuracy: 0.7964\n",
      "Epoch 22/30\n",
      "5453/5453 [==============================] - 87s 16ms/step - loss: 0.6493 - sparse_categorical_accuracy: 0.8089 - val_loss: 0.7325 - val_sparse_categorical_accuracy: 0.7957\n",
      "Epoch 23/30\n",
      "5453/5453 [==============================] - 88s 16ms/step - loss: 0.6554 - sparse_categorical_accuracy: 0.8081 - val_loss: 0.7526 - val_sparse_categorical_accuracy: 0.7978\n",
      "Epoch 24/30\n",
      "5453/5453 [==============================] - 83s 15ms/step - loss: 0.6611 - sparse_categorical_accuracy: 0.8072 - val_loss: 0.7544 - val_sparse_categorical_accuracy: 0.7935\n",
      "Epoch 25/30\n",
      "5453/5453 [==============================] - 74s 13ms/step - loss: 0.6657 - sparse_categorical_accuracy: 0.8067 - val_loss: 0.7519 - val_sparse_categorical_accuracy: 0.7954\n",
      "Epoch 26/30\n",
      "5453/5453 [==============================] - 85s 15ms/step - loss: 0.6723 - sparse_categorical_accuracy: 0.8067 - val_loss: 0.8028 - val_sparse_categorical_accuracy: 0.7951\n",
      "Epoch 27/30\n",
      "5453/5453 [==============================] - 84s 15ms/step - loss: 0.6775 - sparse_categorical_accuracy: 0.8053 - val_loss: 0.7897 - val_sparse_categorical_accuracy: 0.7955\n",
      "Epoch 28/30\n",
      "5453/5453 [==============================] - 79s 14ms/step - loss: 0.6808 - sparse_categorical_accuracy: 0.8052 - val_loss: 0.7930 - val_sparse_categorical_accuracy: 0.7906\n",
      "Epoch 29/30\n",
      "5453/5453 [==============================] - 89s 16ms/step - loss: 0.6861 - sparse_categorical_accuracy: 0.8040 - val_loss: 0.7833 - val_sparse_categorical_accuracy: 0.7959\n",
      "Epoch 30/30\n",
      "5453/5453 [==============================] - 79s 14ms/step - loss: 0.6904 - sparse_categorical_accuracy: 0.8037 - val_loss: 0.7989 - val_sparse_categorical_accuracy: 0.7940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5e404b8520>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(62, \"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(),\n",
    "    #optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    #loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=30,\n",
    "    validation_data=ds_test,\n",
    "    callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath='tfKerasCharsEMNISTSoftmax', save_best_only=True, monitor=\"val_loss\"),\n",
    "               tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 17:36:55.806892: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2022-08-18 17:36:55.806974: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2022-08-18 17:36:55.810073: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: tfKerasCharsEMNISTSoftmax\n",
      "2022-08-18 17:36:55.814254: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2022-08-18 17:36:55.814364: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: tfKerasCharsEMNISTSoftmax\n",
      "2022-08-18 17:36:55.839865: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-08-18 17:36:55.989313: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: tfKerasCharsEMNISTSoftmax\n",
      "2022-08-18 17:36:56.045271: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 235251 microseconds.\n",
      "2022-08-18 17:36:56.214720: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model('tfKerasCharsEMNISTSoftmax')\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"tfKerasCharsEMNISTSoftmax.tflite\", \"wb\") as fp:\n",
    "    fp.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 62)                7998      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,478\n",
      "Trainable params: 108,478\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Element1 Shape (28, 28, 1)\n",
      "Ergebnis Shape (1, 62)\n",
      "Zeichen unmapped 28 Zeichen mapped S Ergebnis 0.88363343\n",
      "Element 28\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "charList = ['0','1','2','3','4','5','6','7','8','9',\n",
    "'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z',\n",
    "'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "\n",
    "model = tf.keras.models.load_model('tfKerasCharsEMNISTSoftmax')\n",
    "model.summary()\n",
    "\n",
    "for element in ds_train.as_numpy_iterator():\n",
    "    element1, element2 = element\n",
    "    ele = np.resize(element1[0],(1,28,28,1))\n",
    "    erg = model.predict(ele)\n",
    "    print(\"Element1 Shape\", element1[0].shape)\n",
    "    print(\"Ergebnis Shape\", erg.shape)\n",
    "    print(\"Zeichen unmapped\", np.argmax(erg), \"Zeichen mapped\", charList[np.argmax(erg)], \"Ergebnis\", erg[0][np.argmax(erg)])\n",
    "    print(\"Element\", element2[0])\n",
    "    plt.imshow(element1[0], cmap='gray') #Bilder sind gedreht und gespiegelt\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 62)                7998      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,478\n",
      "Trainable params: 108,478\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(1, 28, 28)\n",
      "[[1.8807922e-26 1.1773163e-32 2.5462825e-24 0.0000000e+00 0.0000000e+00\n",
      "  5.7181509e-21 0.0000000e+00 2.3309642e-32 2.0760651e-34 6.2360591e-31\n",
      "  1.1094535e-28 1.0451843e-15 6.5116700e-23 1.4345829e-15 0.0000000e+00\n",
      "  7.4750385e-24 0.0000000e+00 3.4343622e-34 1.2634439e-37 3.6869738e-22\n",
      "  2.6603500e-24 3.1928002e-26 8.2443227e-13 9.8905066e-17 1.6949757e-29\n",
      "  1.9004870e-20 1.1458955e-19 1.8752265e-25 4.3013564e-16 9.3880790e-23\n",
      "  1.0000000e+00 4.7501620e-36 6.8860770e-13 8.0936312e-27 2.5871803e-16\n",
      "  6.4009204e-22 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 5.3478207e-30 6.2543970e-33 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 4.5339599e-32 0.0000000e+00 1.7737852e-31\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 6.3628058e-34 0.0000000e+00\n",
      "  2.0837264e-22 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('tfKerasCharsEMNISTSoftmax')\n",
    "model.summary()\n",
    "\n",
    "data = np.random.randint(0, 255, (1, 28, 28))/255\n",
    "\n",
    "print(data.shape)\n",
    "#print(data[1][1])\n",
    "\n",
    "erg = model.predict(data)\n",
    "#print(np.argmax(erg))\n",
    "print(erg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.8809071e-26 1.1772804e-32 2.5463602e-24 0.0000000e+00 0.0000000e+00\n",
      "  5.7179756e-21 0.0000000e+00 2.3308929e-32 2.0759382e-34 6.2358687e-31\n",
      "  1.1094197e-28 1.0451842e-15 6.5120670e-23 1.4344953e-15 0.0000000e+00\n",
      "  7.4748089e-24 0.0000000e+00 3.4343622e-34 1.2633669e-37 3.6868610e-22\n",
      "  2.6602685e-24 3.1925078e-26 8.2440706e-13 9.8908077e-17 1.6950796e-29\n",
      "  1.9003130e-20 1.1458955e-19 1.8751120e-25 4.3014879e-16 9.3877919e-23\n",
      "  1.0000000e+00 4.7501616e-36 6.8858677e-13 8.0936305e-27 2.5868643e-16\n",
      "  6.4011163e-22 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 5.3476571e-30 6.2545880e-33 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 4.5340980e-32 0.0000000e+00 1.7737309e-31\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 6.3624169e-34 0.0000000e+00\n",
      "  2.0836627e-22 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"tfKerasCharsEMNISTSoftmax.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], data.astype(np.float32))\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tensorflow/datasets",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
